------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------Yuyang Luo----------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
problem during data load
1.  null value in the dataset
2.  some dataset include  useless attributes(for this project)
3.  there is a huge dataset. hard to directly load to the team database.
4.  some datasets include information that not Atomicity， processing with Excel to partition these kinds of info in CSV

I use local pgsql database with pgadminIII to load these data with null value processing, then delete useless attributes and make a subset of the huge dataset, finally use pg_dump command to export those tables into SQL script and import these data into the team database.

To load those data, login into pgsql console， and use the “\i xxx.sql” command to execute those scripts.

And an additional problem:
5.  the SQL file generated by pg_dump include full information of the table, with the schema path and other Not portable settings, so I remove the public prefix of table name and all SET commands except the default tablespace settings from the script.
------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------Danyang Ren---------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
1. Never used the python scraper beforeand it took me a lot time to decide in which way to scrape those information and to complete the code.
2. Delete the duplicates and useless information;
3. Convert the csv file into sql file;
